#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AGI Kernel â€” è‡ªå·±æ”¹å–„ãƒ«ãƒ¼ãƒ— (v0.6.3)

ãƒªãƒã‚¸ãƒˆãƒªã®å¥å…¨æ€§ã‚’ã‚¹ã‚­ãƒ£ãƒ³â†’ã‚¿ã‚¹ã‚¯å€™è£œç”Ÿæˆâ†’1ã¤é¸æŠâ†’(å®Ÿè¡Œ/æ¤œè¨¼)â†’å­¦ç¿’è¨˜éŒ²â†’çŠ¶æ…‹ä¿å­˜
ã‚’1ã‚µã‚¤ã‚¯ãƒ«ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹ã€‚

çµ‚äº†ã‚³ãƒ¼ãƒ‰:
  EXIT_SUCCESS  (0) â€” æ­£å¸¸å®Œäº† / FAILUREå¾Œã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œäº†
  EXIT_PAUSED   (1) â€” ã‚¿ã‚¹ã‚¯ãŒ3å›å¤±æ•—ã—PAUSED / env blocker / resume PAUSED
  EXIT_LOCK     (2) â€” åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒ­ãƒƒã‚¯ä¿æŒä¸­

ä½¿ç”¨ä¾‹:
    python agi_kernel.py --once --dry-run
    python agi_kernel.py --resume --dry-run
    python agi_kernel.py --workspaces /repo1 /repo2 --dry-run
"""

from __future__ import annotations

__version__ = "0.6.3"

# â”€â”€ çµ‚äº†ã‚³ãƒ¼ãƒ‰å®šæ•° â”€â”€
EXIT_SUCCESS = 0   # æ­£å¸¸å®Œäº† / FAILUREå¾Œã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œäº†
EXIT_PAUSED = 1    # PAUSEDï¼ˆ3å›å¤±æ•—ï¼‰/ BLOCKED / resume PAUSED
EXIT_LOCK = 2      # åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒ­ãƒƒã‚¯ä¿æŒä¸­

import argparse
import json
import logging
import os
import subprocess
import sys
import time
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Any, Optional

# â”€â”€ .env è‡ªå‹•èª­ã¿è¾¼ã¿ â”€â”€
try:
    from dotenv import load_dotenv
    _dotenv_path = os.path.join(os.path.dirname(__file__), "..", "..", "..", ".env")
    if os.path.isfile(_dotenv_path):
        load_dotenv(_dotenv_path, override=False)
except ImportError:
    pass

# â”€â”€ ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ– â”€â”€
logger = logging.getLogger("agi_kernel")

# â”€â”€ ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« import â”€â”€
from state import (  # noqa: E402
    JST,
    FAILURE_CATEGORIES,
    MAX_TASK_FAILURES,
    LOCK_TTL_SECONDS,
    FileLock,
    StateManager,
    classify_failure,
    record_failure,
    record_ki,
)
from scanner import (  # noqa: E402
    strip_ansi,
    parse_pytest_result,
    Scanner,
    generate_candidates,
    annotate_candidates,
    select_task,
    _extract_error_blocks,
    _extract_failure_nodes,
    _stable_task_id,
)
from executor import (  # noqa: E402
    MAX_PATCH_FILES,
    MAX_DIFF_LINES,
    MAX_LLM_RETRIES,
    COMMAND_ALLOWLIST,
    _COST_PER_1M,
    GeminiClient,
    get_genai_client,
    log_token_usage,
    Executor,
    GeminiExecutor,
    parse_patch_json,
    validate_patch_result,
    apply_patch,
    preflight_check,
    backup_targets,
    rollback_with_backup,
    restore_rollback_context,
    compute_patch_diff_lines,
    build_execute_context,
)
from verifier import Verifier  # noqa: E402
from webhook import send_webhook  # noqa: E402

# â”€â”€ å¾Œæ–¹äº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆãƒ†ã‚¹ãƒˆãƒ»å¤–éƒ¨ã‚³ãƒ¼ãƒ‰å‘ã‘ï¼‰ â”€â”€
# v0.6.0ä»¥å‰ã¯å…¨é–¢æ•°ãŒ agi_kernel.py ã« _prefix ä»˜ãã§å®šç¾©ã•ã‚Œã¦ã„ãŸã€‚
# æ—¢å­˜ãƒ†ã‚¹ãƒˆãŒã“ã‚Œã‚‰ã®åå‰ã§importã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ç¶­æŒã™ã‚‹ã€‚
_validate_patch_result = validate_patch_result
_apply_patch = apply_patch
_parse_patch_json = parse_patch_json
_preflight_check = preflight_check
_backup_targets = backup_targets
_rollback_with_backup = rollback_with_backup
_compute_patch_diff_lines = compute_patch_diff_lines
_restore_rollback_context = restore_rollback_context
_log_token_usage = log_token_usage
_record_ki = record_ki
_send_webhook = send_webhook
_GeminiClientCompat = GeminiClient  # æ—§å
COST_PER_1M = _COST_PER_1M  # ãƒ†ã‚¹ãƒˆäº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆãƒ‘ãƒ–ãƒªãƒƒã‚¯åï¼‰

# â”€â”€ WorkflowLoggerçµ±åˆ â”€â”€
_HAS_LOGGER = False
try:
    _SCRIPT_DIR = Path(__file__).resolve().parent
    sys.path.insert(0, str(_SCRIPT_DIR.parents[2] / ".agent" / "workflows" / "shared"))
    from workflow_logger import run_logged_main  # type: ignore
    _HAS_LOGGER = True
except ImportError:
    pass


# ============================================================
# JSONæ§‹é€ åŒ–ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿
# ============================================================

class _JsonFormatter(logging.Formatter):
    """JSONæ§‹é€ åŒ–ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã€‚"""
    def format(self, record: logging.LogRecord) -> str:
        entry = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "message": record.getMessage(),
        }
        if hasattr(record, "phase"):
            entry["phase"] = record.phase
        return json.dumps(entry, ensure_ascii=False, default=str)


def _setup_logging(*, json_mode: bool = False, level: int = logging.INFO) -> None:
    """ãƒ­ã‚®ãƒ³ã‚°åˆæœŸè¨­å®šã€‚json_mode=True ã§JSONæ§‹é€ åŒ–å‡ºåŠ›ã€‚"""
    root_logger = logging.getLogger("agi_kernel")
    root_logger.setLevel(level)
    # æ—¢å­˜ãƒãƒ³ãƒ‰ãƒ©ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†è¨­å®šï¼ˆãƒ†ã‚¹ãƒˆæ™‚ã®å†å‘¼ã³å‡ºã—ã«å¯¾å¿œï¼‰
    root_logger.handlers.clear()
    handler = logging.StreamHandler()
    if json_mode:
        handler.setFormatter(_JsonFormatter())
    else:
        handler.setFormatter(logging.Formatter("%(message)s"))
    root_logger.addHandler(handler)


# ============================================================
# ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ«ãƒ¼ãƒˆã®è§£æ±º
# ============================================================

_SCRIPT_DIR = Path(__file__).resolve().parent
_DEFAULT_WORKSPACE = _SCRIPT_DIR.parents[2]  # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ/AGIã‚«ãƒ¼ãƒãƒ«/scripts â†’ root


# ============================================================
# ãƒ•ã‚§ãƒ¼ã‚ºé †åºå®šç¾©ï¼ˆ--resume ç”¨ï¼‰
# ============================================================

PHASE_ORDER = ["BOOT", "SCAN", "SENSE", "SELECT", "EXECUTE", "VERIFY", "LEARN", "CHECKPOINT"]


def _should_skip_phase(last_completed: str, target_phase: str) -> bool:
    """resumeæ™‚ã€target_phaseãŒæ—¢ã«å®Œäº†æ¸ˆã¿ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚"""
    try:
        completed_idx = PHASE_ORDER.index(last_completed)
        target_idx = PHASE_ORDER.index(target_phase)
    except ValueError:
        return False
    return target_idx <= completed_idx


# ============================================================
# ãƒ¡ã‚¤ãƒ³ã‚µã‚¤ã‚¯ãƒ«
# ============================================================

def run_cycle(args: argparse.Namespace, workspace: Path | None = None) -> int:
    """1ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

    Args:
        args: CLIãƒ‘ãƒ¼ã‚¹æ¸ˆã¿å¼•æ•°
        workspace: å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ï¼ˆNoneãªã‚‰args.workspaceã‚’ä½¿ç”¨ï¼‰
    """
    ws = workspace or Path(args.workspace).resolve()
    output_dir = ws / "_outputs" / "agi_kernel"
    sm = StateManager(output_dir)

    # â”€â”€ LOCK â”€â”€
    lock = FileLock(output_dir / "lock")
    if not lock.acquire():
        logger.warning("[LOCK] åˆ¥ã®AGI Kernelãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿè¡Œä¸­ã§ã™ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        return EXIT_LOCK

    try:
        return _run_cycle_inner(args, ws, output_dir, sm)
    finally:
        lock.release()


def _run_cycle_inner(
    args: argparse.Namespace,
    workspace: Path,
    output_dir: Path,
    sm: StateManager,
) -> int:
    """ãƒ­ãƒƒã‚¯å–å¾—å¾Œã®å†…éƒ¨ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œã€‚"""
    resume_phase: Optional[str] = None

    # â”€â”€ BOOT â”€â”€
    if args.resume:
        state = sm.load()
        if state is None:
            logger.info("[BOOT] state.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ã‚µã‚¤ã‚¯ãƒ«ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            state = sm.new_state()
        else:
            logger.info(f"[BOOT] state.jsonã‹ã‚‰å†é–‹: cycle_id={state['cycle_id']}, phase={state['phase']}")
            if state.get("status") == "PAUSED":
                logger.warning("[BOOT] ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒPAUSEDã§ã™ã€‚æ‰‹å‹•ã§ãƒªã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚")
                return EXIT_PAUSED
            if state.get("status") == "COMPLETED":
                logger.info("[BOOT] å‰å›ã‚µã‚¤ã‚¯ãƒ«ã¯å®Œäº†æ¸ˆã¿ã€‚æ–°è¦ã‚µã‚¤ã‚¯ãƒ«ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
                state = sm.new_state()
            else:
                resume_phase = state.get("last_completed_phase")
                if resume_phase is None:
                    old_phase = state.get("phase", "BOOT")
                    try:
                        idx = PHASE_ORDER.index(old_phase)
                        resume_phase = PHASE_ORDER[idx - 1] if idx > 0 else None
                    except ValueError:
                        resume_phase = None
                if resume_phase:
                    logger.info(f"[BOOT] last_completed_phase={resume_phase} ã®æ¬¡ã‹ã‚‰å†é–‹ã—ã¾ã™ã€‚")
                else:
                    logger.info("[BOOT] å®Œäº†æ¸ˆã¿ãƒ•ã‚§ãƒ¼ã‚ºãªã—ã€‚æœ€åˆã‹ã‚‰å®Ÿè¡Œã—ã¾ã™ã€‚")
    else:
        state = sm.new_state()

    state["phase"] = "BOOT"
    state["last_completed_phase"] = None
    state["status"] = "RUNNING"
    date_str = datetime.now(JST).strftime("%Y%m%d")

    logger.info(f"[BOOT] ã‚µã‚¤ã‚¯ãƒ«é–‹å§‹: cycle_id={state['cycle_id']}")
    state["last_completed_phase"] = "BOOT"
    sm.save(state)

    # â”€â”€ SCAN â”€â”€
    if not (resume_phase and _should_skip_phase(resume_phase, "SCAN")):
        state["phase"] = "SCAN"
        logger.info("[SCAN] ãƒªãƒã‚¸ãƒˆãƒªã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
        scanner = Scanner(workspace)
        sev_raw = getattr(args, "lint_severity", "error")
        sev_filter = tuple(f"[{s.strip().upper()}]" for s in sev_raw.split(","))
        lint_result = scanner.run_workflow_lint(severity_filter=sev_filter)
        pytest_result = scanner.run_pytest()
        _lint_errors = max(0, lint_result.get("errors", 0))
        _pytest_errors = max(0, pytest_result.get("errors_count", 0))
        _pytest_failures = max(0, pytest_result.get("failures", 0))
        state["scan_results"] = {
            "workflow_lint": lint_result,
            "pytest": pytest_result,
            "workflow_lint_errors": _lint_errors,
            "pytest_errors": _pytest_errors,
            "pytest_failures": _pytest_failures,
            "total_issues": _lint_errors + _pytest_errors + _pytest_failures,
        }
        logger.info(f"[SCAN] lint_errors={_lint_errors}, pytest_errors={_pytest_errors}, pytest_failures={_pytest_failures}")
        state["last_completed_phase"] = "SCAN"
        sm.save(state)
    else:
        logger.info("[SCAN] resume: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Œäº†æ¸ˆã¿ï¼‰")

    # â”€â”€ SENSE â”€â”€
    if not (resume_phase and _should_skip_phase(resume_phase, "SENSE")):
        state["phase"] = "SENSE"
        candidates = generate_candidates(state["scan_results"])
        state["candidates"] = candidates
        annotate_candidates(candidates)
        sm.save_candidates(candidates, date_str, state["cycle_id"])
        logger.info(f"[SENSE] ã‚¿ã‚¹ã‚¯å€™è£œ: {len(candidates)}ä»¶")
        blocked = [c for c in candidates if not c.get("auto_fixable", True)]
        fixable = [c for c in candidates if c.get("auto_fixable", True)]
        if blocked:
            logger.info(f"[SENSE] auto_fixable=false: {len(blocked)}ä»¶ (blocked)")
        logger.info(f"[SENSE] auto_fixable=true: {len(fixable)}ä»¶ (å¯¾å‡¦å¯èƒ½)")
        state["last_completed_phase"] = "SENSE"
        sm.save(state)
    else:
        logger.info("[SENSE] resume: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Œäº†æ¸ˆã¿ï¼‰")
        candidates = state.get("candidates", [])

    # â”€â”€ SELECT â”€â”€
    if not (resume_phase and _should_skip_phase(resume_phase, "SELECT")):
        state["phase"] = "SELECT"
        selected = select_task(candidates, state.get("paused_tasks", []))
        state["selected_task"] = selected
        if selected is None:
            blocked = [c for c in candidates if not c.get("auto_fixable", True)]
            reason = "no_fixable_candidates" if blocked else "no_candidates"
            logger.info(f"[SELECT] å¯¾å‡¦å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ{reason}ï¼‰ã€‚ã‚µã‚¤ã‚¯ãƒ«å®Œäº†ã€‚")
            state["status"] = "COMPLETED"
            state["completed_at"] = datetime.now(JST).isoformat()
            state["phase"] = "CHECKPOINT"
            state["last_completed_phase"] = "CHECKPOINT"
            sm.save(state)
            report = {
                "cycle_id": state["cycle_id"],
                "status": state["status"],
                "reason": reason,
                "scan_summary": {
                    "lint_errors": state["scan_results"].get("workflow_lint_errors", 0),
                    "pytest_errors": state["scan_results"].get("pytest_errors", 0),
                    "pytest_failures": state["scan_results"].get("pytest_failures", 0),
                },
                "candidates_count": len(candidates),
                "blocked_candidates": [
                    {"task_id": c["task_id"], "title": c["title"], "blocked_reason": c.get("blocked_reason", "")}
                    for c in blocked
                ],
                "selected_task": None,
                "outcome": "SUCCESS",
                "paused_tasks": state.get("paused_tasks", []),
            }
            sm.save_report(report, date_str, state["cycle_id"])
            record_ki("SUCCESS", cycle_id=state["cycle_id"], task_id="none", note=reason)
            logger.info(f"[CHECKPOINT] stateä¿å­˜å®Œäº†: {sm.state_path}")
            return 0
        logger.info(f"[SELECT] ã‚¿ã‚¹ã‚¯é¸æŠ: {selected['task_id']} â€” {selected['title']}")
        state["last_completed_phase"] = "SELECT"
        sm.save(state)
    else:
        logger.info("[SELECT] resume: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Œäº†æ¸ˆã¿ï¼‰")
        selected = state.get("selected_task")

    # â”€â”€ EXECUTE â”€â”€
    modified_paths: list[Path] = []
    backup_map: dict[str, Optional[Path]] = {}
    if not (resume_phase and _should_skip_phase(resume_phase, "EXECUTE")):
        state["phase"] = "EXECUTE"
        if args.dry_run:
            logger.info("[EXECUTE] dry-runãƒ¢ãƒ¼ãƒ‰: ã‚¹ã‚­ãƒƒãƒ—")
            state["execution_result"] = {"dry_run": True, "skipped": True}
        else:
            # â”€â”€ Preflight â”€â”€
            preflight = preflight_check(workspace)
            if not preflight["ok"]:
                reason = preflight["reason"]
                logger.error(f"[EXECUTE] âŒ Preflightå¤±æ•— (ç’°å¢ƒãƒ–ãƒ­ãƒƒã‚«ãƒ¼): {reason}")
                state["status"] = "PAUSED"
                state["completed_at"] = datetime.now(JST).isoformat()
                state["phase"] = "CHECKPOINT"
                state["last_completed_phase"] = "CHECKPOINT"
                sm.save(state)
                report = {
                    "cycle_id": state["cycle_id"],
                    "status": "PAUSED",
                    "reason": f"blocked_by_{reason}",
                    "scan_summary": {
                        "lint_errors": state["scan_results"].get("workflow_lint_errors", 0),
                        "pytest_errors": state["scan_results"].get("pytest_errors", 0),
                        "pytest_failures": state["scan_results"].get("pytest_failures", 0),
                    },
                    "candidates_count": len(candidates),
                    "selected_task": selected,
                    "outcome": "BLOCKED",
                    "paused_tasks": state.get("paused_tasks", []),
                }
                sm.save_report(report, date_str, state["cycle_id"])
                record_ki("FAILURE", cycle_id=state["cycle_id"],
                          task_id=selected["task_id"] if selected else "none",
                          note=f"env_blocker:{reason}")
                return EXIT_PAUSED
            else:
                if not preflight["git_available"]:
                    logger.warning("[EXECUTE] âš ï¸ gitä¸åœ¨ â€” difflibãƒ™ãƒ¼ã‚¹ã§å®‰å…¨å¼ã‚’é©ç”¨")

                # â”€â”€ LLMãƒ‘ãƒƒãƒç”Ÿæˆâ†’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—â†’é©ç”¨â†’diffæ¤œè¨¼ â”€â”€
                logger.info("[EXECUTE] LLMãƒ‘ãƒƒãƒç”Ÿæˆã‚’é–‹å§‹...")
                try:
                    model_name = getattr(args, "llm_model", None) or "gemini-2.5-flash"
                    strong_name = getattr(args, "llm_strong_model", None) or "gemini-2.5-pro"
                    executor = GeminiExecutor(
                        model_name=model_name,
                        strong_model_name=strong_name,
                        state=state,
                    )
                    context = build_execute_context(selected, state["scan_results"], workspace)
                    patch = executor.generate_patch(selected, context, workspace)
                    logger.info(f"[EXECUTE] ãƒ‘ãƒƒãƒç”Ÿæˆå®Œäº†: {len(patch['files'])}ãƒ•ã‚¡ã‚¤ãƒ«")
                    logger.info(f"[EXECUTE] èª¬æ˜: {patch.get('explanation', '')[:200]}")

                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
                    date_str = datetime.now(JST).strftime("%Y%m%d")
                    bak_dir = output_dir / date_str / state["cycle_id"] / "backup"
                    backup_map = backup_targets(patch, workspace, bak_dir)
                    logger.info(f"[EXECUTE] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {bak_dir}")

                    # --approve ã‚²ãƒ¼ãƒˆ
                    if getattr(args, "approve", False):
                        logger.info("=" * 60)
                        logger.info("[APPROVE] ãƒ‘ãƒƒãƒå†…å®¹:")
                        for f in patch["files"]:
                            logger.info(f"  {f.get('action', 'modify')}: {f['path']}")
                        logger.info(f"  èª¬æ˜: {patch.get('explanation', '')[:300]}")
                        logger.info("=" * 60)
                        answer = input("[APPROVE] é©ç”¨ã—ã¾ã™ã‹? (y/n): ").strip().lower()
                        if answer != "y":
                            logger.info("[APPROVE] ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‹’å¦ã€‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
                            state["execution_result"] = {"success": False, "error": "user_rejected"}
                            state["last_completed_phase"] = "EXECUTE"
                            sm.save(state)
                            modified_paths = []
                            resume_phase = "EXECUTE"
                            state["verification_result"] = {"success": False, "skipped": True}
                            state["last_completed_phase"] = "VERIFY"
                            sm.save(state)

                    # ãƒ‘ãƒƒãƒé©ç”¨
                    modified_paths = apply_patch(patch, workspace)
                    logger.info(f"[EXECUTE] ãƒ‘ãƒƒãƒé©ç”¨å®Œäº†: {[str(p.relative_to(workspace)) for p in modified_paths]}")

                    # diffè¡Œæ•°ãƒã‚§ãƒƒã‚¯
                    diff_lines = compute_patch_diff_lines(patch, backup_map)
                    logger.info(f"[EXECUTE] diffè¡Œæ•°: {diff_lines}")
                    if diff_lines > MAX_DIFF_LINES:
                        logger.warning(f"[EXECUTE] diffè¡Œæ•° {diff_lines} > ä¸Šé™ {MAX_DIFF_LINES}ã€‚ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                        rollback_with_backup(modified_paths, backup_map, workspace)
                        modified_paths = []
                        state["execution_result"] = {
                            "success": False,
                            "error": f"diffè¡Œæ•°è¶…é: {diff_lines} > {MAX_DIFF_LINES}",
                            "patch_explanation": patch.get("explanation", ""),
                        }
                    else:
                        state["execution_result"] = {
                            "success": True,
                            "files_modified": len(modified_paths),
                            "diff_lines": diff_lines,
                            "patch_explanation": patch.get("explanation", ""),
                            "git_available": preflight["git_available"],
                            "modified_files": [
                                str(p.relative_to(workspace)).replace("\\", "/")
                                for p in modified_paths
                            ],
                            "backup_dir": str(
                                bak_dir.relative_to(output_dir)
                            ).replace("\\", "/"),
                        }
                except RuntimeError as e:
                    logger.error(f"[EXECUTE] ã‚¨ãƒ©ãƒ¼: {e}")
                    if modified_paths:
                        rollback_with_backup(modified_paths, backup_map, workspace)
                        modified_paths = []
                    state["execution_result"] = {"success": False, "error": str(e)}
                except Exception as e:
                    logger.error(f"[EXECUTE] äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                    if modified_paths:
                        rollback_with_backup(modified_paths, backup_map, workspace)
                        modified_paths = []
                    state["execution_result"] = {"success": False, "error": str(e)}

        state["last_completed_phase"] = "EXECUTE"
        sm.save(state)
    else:
        logger.info("[EXECUTE] resume: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Œäº†æ¸ˆã¿ï¼‰")

    # â”€â”€ VERIFY â”€â”€
    if not (resume_phase and _should_skip_phase(resume_phase, "VERIFY")):
        state["phase"] = "VERIFY"
        exec_result = state.get("execution_result", {})

        if not modified_paths and exec_result.get("modified_files"):
            modified_paths, backup_map = restore_rollback_context(
                state, workspace, output_dir,
            )
            if modified_paths:
                logger.info(f"[VERIFY] ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’stateã‹ã‚‰å¾©å…ƒ ({len(modified_paths)}ãƒ•ã‚¡ã‚¤ãƒ«)")

        if args.dry_run:
            logger.info("[VERIFY] dry-runãƒ¢ãƒ¼ãƒ‰: ã‚¹ã‚­ãƒƒãƒ—")
            state["verification_result"] = {"dry_run": True, "skipped": True}
        elif not exec_result.get("success", False):
            logger.info("[VERIFY] EXECUTEå¤±æ•—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            state["verification_result"] = {"skipped": True, "reason": "execute_failed"}
        else:
            logger.info("[VERIFY] æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œä¸­...")
            verifier = Verifier(workspace)
            verify_result = verifier.verify(selected)
            state["verification_result"] = verify_result
            if verify_result["success"]:
                logger.info(f"[VERIFY] âœ… æ¤œè¨¼æˆåŠŸ (exit_code={verify_result['exit_code']})")
                exec_git = state.get("execution_result", {}).get("git_available", False)
                if getattr(args, "auto_commit", False) and exec_git:
                    try:
                        subprocess.run(
                            ["git", "add", "-A"],
                            cwd=str(workspace),
                            capture_output=True, text=True, timeout=10,
                        )
                        task_id = selected.get("task_id", "unknown") if selected else "unknown"
                        subprocess.run(
                            ["git", "commit", "-m", f"[AGI-Kernel] auto-fix: {task_id}"],
                            cwd=str(workspace),
                            capture_output=True, text=True, timeout=10,
                        )
                        logger.info("[VERIFY] ğŸ”’ auto-commit å®Œäº†")
                        state["verification_result"]["auto_committed"] = True
                    except (FileNotFoundError, subprocess.TimeoutExpired, OSError) as ce:
                        logger.warning(f"[VERIFY] âš ï¸ auto-commit å¤±æ•—: {ce}")
                elif exec_git:
                    logger.info("[VERIFY] âš ï¸ VERIFYæˆåŠŸã€‚æ¬¡ã‚µã‚¤ã‚¯ãƒ«å®‰å®šåŒ–ã®ãŸã‚æ‰‹å‹•commitã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
            else:
                logger.warning(f"[VERIFY] âŒ æ¤œè¨¼å¤±æ•— (exit_code={verify_result['exit_code']})")
                logger.warning(f"[VERIFY] å‡ºåŠ›: {verify_result['output'][:500]}")
                if modified_paths:
                    logger.info("[VERIFY] å¤‰æ›´ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
                    rollback_with_backup(modified_paths, backup_map, workspace)
                    state["verification_result"]["rolled_back"] = True

        state["last_completed_phase"] = "VERIFY"
        sm.save(state)
    else:
        logger.info("[VERIFY] resume: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Œäº†æ¸ˆã¿ï¼‰")

    # â”€â”€ LEARN â”€â”€
    paused_now = False
    if not (resume_phase and _should_skip_phase(resume_phase, "LEARN")):
        state["phase"] = "LEARN"
        category = ""
        error_msg = ""
        exec_result = state.get("execution_result", {})
        verify_result = state.get("verification_result", {})
        if args.dry_run:
            outcome = "PARTIAL"
            note = "dry_run"
        elif verify_result.get("success", False):
            outcome = "SUCCESS"
            note = "auto_fix_verified"
        elif exec_result.get("success", False) and not verify_result.get("success", False):
            outcome = "FAILURE"
            note = "verify_failed"
            error_msg = verify_result.get("output", "verification failed")[:500]
            category = classify_failure(error_msg)
            paused_now = record_failure(state, selected["task_id"], category, error_msg)
        else:
            outcome = "FAILURE"
            note = "execute_failed"
            error_msg = exec_result.get("error", "execute failed")[:500]
            category = classify_failure(error_msg)
            paused_now = record_failure(state, selected["task_id"], category, error_msg)

        record_ki(
            outcome=outcome,
            cycle_id=state["cycle_id"],
            task_id=selected["task_id"] if selected else "none",
            note=note,
            metadata={
                "failure_class": category if outcome == "FAILURE" else None,
                "error_summary": (error_msg[:200] if outcome == "FAILURE" else None),
                "verification_success": verify_result.get("success", None),
                "files_modified": exec_result.get("files_modified", 0),
            },
        )
        logger.info(f"[LEARN] KI Learningè¨˜éŒ²: outcome={outcome}, note={note}")
        state["last_completed_phase"] = "LEARN"
        sm.save(state)
    else:
        logger.info("[LEARN] resume: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Œäº†æ¸ˆã¿ï¼‰")
        outcome = state.get("execution_result", {}).get("success", False) and \
                  state.get("verification_result", {}).get("success", False)
        outcome = "SUCCESS" if outcome else "PARTIAL"
        if selected and selected.get("task_id") in state.get("paused_tasks", []):
            paused_now = True

    # â”€â”€ CHECKPOINT â”€â”€
    state["phase"] = "CHECKPOINT"
    state["last_completed_phase"] = "CHECKPOINT"
    state["completed_at"] = datetime.now(JST).isoformat()

    paused_now_flag = paused_now
    if paused_now_flag:
        state["status"] = "PAUSED"
        logger.warning(f"[CHECKPOINT] âš ï¸ ã‚¿ã‚¹ã‚¯ {selected['task_id']} ãŒ {MAX_TASK_FAILURES}å›å¤±æ•— â†’ PAUSEDåœæ­¢")
    else:
        state["status"] = "COMPLETED"
    sm.save(state)

    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    blocked = [c for c in candidates if not c.get("auto_fixable", True)]
    report = {
        "cycle_id": state["cycle_id"],
        "status": state["status"],
        "scan_summary": {
            "lint_errors": state["scan_results"].get("workflow_lint_errors", 0),
            "pytest_errors": state["scan_results"].get("pytest_errors", 0),
            "pytest_failures": state["scan_results"].get("pytest_failures", 0),
        },
        "candidates_count": len(candidates),
        "blocked_candidates": [
            {"task_id": c["task_id"], "title": c["title"], "blocked_reason": c.get("blocked_reason", "")}
            for c in blocked
        ],
        "selected_task": selected,
        "outcome": outcome,
        "paused_tasks": state.get("paused_tasks", []),
        "token_usage": state.get("token_usage", {}),
    }
    sm.save_report(report, date_str, state["cycle_id"])
    logger.info(f"[CHECKPOINT] stateä¿å­˜å®Œäº†: {sm.state_path}")

    # Webhooké€šçŸ¥ (v0.6.1: å …ç‰¢åŒ–ç‰ˆ)
    webhook_url = getattr(args, "webhook_url", None)
    if webhook_url:
        send_webhook(webhook_url, {
            "summary": f"AGI Kernel: cycle={state['cycle_id']} status={state['status']} outcome={outcome}",
            "cycle_id": state["cycle_id"],
            "status": state["status"],
            "outcome": outcome,
            "token_usage": state.get("token_usage", {}),
        }, cycle_id=state["cycle_id"])

    return EXIT_PAUSED if paused_now_flag else EXIT_SUCCESS


# ============================================================
# CLI
# ============================================================

def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="AGI Kernel â€” è‡ªå·±æ”¹å–„ãƒ«ãƒ¼ãƒ— (v0.6.3)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--once", action="store_true",
        help="1ã‚µã‚¤ã‚¯ãƒ«ã®ã¿å®Ÿè¡Œã—ã¦çµ‚äº†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œï¼‰",
    )
    parser.add_argument(
        "--loop", action="store_true",
        help="å¸¸é§ãƒ¢ãƒ¼ãƒ‰: --interval ç§’ã”ã¨ã«ã‚µã‚¤ã‚¯ãƒ«ã‚’ç¹°ã‚Šè¿”ã™",
    )
    parser.add_argument(
        "--interval", type=int, default=300,
        help="--loop æ™‚ã®ã‚µã‚¤ã‚¯ãƒ«é–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300ï¼‰",
    )
    parser.add_argument(
        "--resume", action="store_true",
        help="state.jsonã‹ã‚‰å†é–‹",
    )
    parser.add_argument(
        "--dry-run", action="store_true", dest="dry_run",
        help="EXECUTE/VERIFYãƒ•ã‚§ãƒ¼ã‚ºã‚’ã‚¹ã‚­ãƒƒãƒ—",
    )
    parser.add_argument(
        "--auto-commit", action="store_true", dest="auto_commit",
        help="VERIFYæˆåŠŸæ™‚ã«è‡ªå‹•commitã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆOFFï¼‰",
    )
    parser.add_argument(
        "--approve", action="store_true",
        help="ãƒ‘ãƒƒãƒé©ç”¨å‰ã«äººé–“ã®æ‰¿èªã‚’è¦æ±‚ã™ã‚‹",
    )
    parser.add_argument(
        "--workspace", type=str, default=str(_DEFAULT_WORKSPACE),
        help=f"ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ«ãƒ¼ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {_DEFAULT_WORKSPACE}ï¼‰",
    )
    # P3-a: ãƒãƒ«ãƒãƒªãƒå¯¾å¿œ
    parser.add_argument(
        "--workspaces", type=str, nargs="+", default=None,
        help="è¤‡æ•°ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å·¡å›ï¼ˆä¾‹: --workspaces /repo1 /repo2ï¼‰",
    )
    parser.add_argument(
        "--llm-model", type=str, default=None, dest="llm_model",
        help="LLMãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.5-flash / env AGI_KERNEL_LLM_MODELï¼‰",
    )
    parser.add_argument(
        "--llm-strong-model", type=str, default=None, dest="llm_strong_model",
        help="å¼·åŠ›LLMãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.5-pro / env AGI_KERNEL_LLM_STRONG_MODELï¼‰",
    )
    parser.add_argument(
        "--webhook-url", type=str, default=None, dest="webhook_url",
        help="ã‚µã‚¤ã‚¯ãƒ«å®Œäº†/PAUSEDæ™‚ã«Webhooké€šçŸ¥ã‚’é€ã‚‹URLï¼ˆDiscord/Slackäº’æ›ï¼‰",
    )
    parser.add_argument(
        "--lint-severity", type=str, default="error", dest="lint_severity",
        help="workflow_lintå–å¾—ãƒ¬ãƒ™ãƒ«ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š: error,caution,advisory,warningï¼‰",
    )
    parser.add_argument(
        "--log-json", action="store_true", dest="log_json",
        help="ãƒ­ã‚°å‡ºåŠ›ã‚’JSONæ§‹é€ åŒ–å½¢å¼ã«ã™ã‚‹",
    )
    return parser


def main() -> int:
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€‚"""
    parser = build_parser()
    args = parser.parse_args()

    # ãƒ­ã‚®ãƒ³ã‚°åˆæœŸåŒ–
    _setup_logging(json_mode=getattr(args, "log_json", False))

    # P3-a: ãƒãƒ«ãƒãƒªãƒå¯¾å¿œ
    workspaces: list[Path] = []
    if args.workspaces:
        workspaces = [Path(w).resolve() for w in args.workspaces]
    else:
        workspaces = [Path(args.workspace).resolve()]

    if args.loop:
        # å¸¸é§ãƒ¢ãƒ¼ãƒ‰
        logger.info(f"[KERNEL] å¸¸é§ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ (interval={args.interval}s, workspaces={len(workspaces)})")
        cycle_count = 0
        try:
            while True:
                cycle_count += 1
                for ws_idx, ws in enumerate(workspaces):
                    ws_label = f"[{ws_idx+1}/{len(workspaces)}] {ws.name}" if len(workspaces) > 1 else ""
                    logger.info(f"[KERNEL] === ã‚µã‚¤ã‚¯ãƒ« #{cycle_count} é–‹å§‹ {ws_label}===")
                    exit_code = run_cycle(args, workspace=ws)
                    if exit_code != 0:
                        logger.warning(f"[KERNEL] ã‚µã‚¤ã‚¯ãƒ« #{cycle_count} {ws_label} ãŒ exit_code={exit_code} ã§çµ‚äº†ã€‚")
                        if len(workspaces) == 1:
                            return exit_code
                        # ãƒãƒ«ãƒãƒªãƒ: 1ã¤å¤±æ•—ã—ã¦ã‚‚æ¬¡ã¸é€²ã‚€
                        continue
                    logger.info(f"[KERNEL] ã‚µã‚¤ã‚¯ãƒ« #{cycle_count} {ws_label} å®Œäº†ã€‚")
                logger.info(f"[KERNEL] å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹å®Œäº†ã€‚{args.interval}ç§’å¾Œã«æ¬¡ã®ã‚µã‚¤ã‚¯ãƒ«...")
                args.resume = False
                time.sleep(args.interval)
        except KeyboardInterrupt:
            logger.info(f"[KERNEL] Ctrl+C ã‚’å—ä¿¡ã€‚{cycle_count}ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œå¾Œã«çµ‚äº†ã€‚")
            return 0
    else:
        # å˜ç™ºãƒ¢ãƒ¼ãƒ‰: å…¨ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å·¡å›
        final_exit = 0
        for ws_idx, ws in enumerate(workspaces):
            if len(workspaces) > 1:
                logger.info(f"[KERNEL] ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ [{ws_idx+1}/{len(workspaces)}]: {ws}")
            exit_code = run_cycle(args, workspace=ws)
            if exit_code != 0:
                final_exit = exit_code
                if len(workspaces) == 1:
                    return exit_code
        return final_exit


if __name__ == "__main__":
    if _HAS_LOGGER:
        exit_code = run_logged_main(
            agent="agi_kernel",
            workflow="agi_kernel",
            main_func=main,
            phase_name="AGI_KERNEL_CYCLE",
        )
        raise SystemExit(exit_code)
    else:
        raise SystemExit(main())
