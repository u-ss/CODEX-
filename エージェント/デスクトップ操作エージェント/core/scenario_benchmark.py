"""
Scenario Benchmarkï¼ˆã‚·ãƒŠãƒªã‚ªãƒ™ãƒ³ãƒï¼‰

ç›®çš„: è©•ä¾¡ãƒ»å›å¸°ãƒ†ã‚¹ãƒˆåŸºç›¤ï¼ˆåˆè¦‹å¯¾å¿œ/çŠ¶æ³æŠŠæ¡ã®æ”¹å–„ã‚’æ¸¬å®šï¼‰

ChatGPT 5.2ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆ2026-02-05 Round6ï¼‰ã‚ˆã‚Š:
ã€Œåˆè¦‹å¯¾å¿œ/çŠ¶æ³æŠŠæ¡ã¯ã€å®Ÿè£…ã‚’å¢—ã‚„ã™ã‚ˆã‚Šã€ä½•ãŒã©ã‚Œã ã‘æ”¹å–„ã—ãŸã‹ã€ã‚’ç¶™ç¶šæ¸¬å®šã§ãã‚‹ã‹ã§æ±ºã¾ã‚Šã¾ã™ã€

è¨­è¨ˆ:
- ã‚·ãƒŠãƒªã‚ªãƒ™ãƒ³ãƒ: åˆè¦‹ã‚¢ãƒ—ãƒª/åˆè¦‹ç”»é¢ã‚’å«ã‚€ã‚¿ã‚¹ã‚¯ã‚»ãƒƒãƒˆ
- æŒ‡æ¨™ï¼ˆè‡ªå‹•é›†è¨ˆï¼‰: ç¢ºåº¦æ¨ç§»/èª¤èªå›æ•°/probeå›æ•°/è¦³æ¸¬ã‚³ã‚¹ãƒˆ
- ãƒªãƒ—ãƒ¬ã‚¤: åŒã˜ãƒ­ã‚°ãƒ»åŒã˜è¦³æ¸¬ã§å†å®Ÿè¡Œã—ã¦å›å¸°ç¢ºèª
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Optional, Any, Callable
import json
import time
from pathlib import Path


class ScenarioType(Enum):
    """ã‚·ãƒŠãƒªã‚ªã‚¿ã‚¤ãƒ—"""
    KNOWN_APP = "known_app"       # æ—¢çŸ¥ã‚¢ãƒ—ãƒª
    UNKNOWN_APP = "unknown_app"   # æœªçŸ¥ã‚¢ãƒ—ãƒªï¼ˆåˆè¦‹ï¼‰
    KNOWN_FLOW = "known_flow"     # æ—¢çŸ¥ãƒ•ãƒ­ãƒ¼
    UNKNOWN_FLOW = "unknown_flow" # æœªçŸ¥ãƒ•ãƒ­ãƒ¼


class TaskStep(Enum):
    """ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒƒãƒ—ã‚¿ã‚¤ãƒ—"""
    NAVIGATE = "navigate"     # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
    CLICK = "click"           # ã‚¯ãƒªãƒƒã‚¯
    INPUT = "input"           # å…¥åŠ›
    WAIT = "wait"             # å¾…æ©Ÿ
    VERIFY = "verify"         # æ¤œè¨¼
    SEARCH = "search"         # æ¤œç´¢


@dataclass
class BenchmarkStep:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ãƒ†ãƒƒãƒ—"""
    step_type: TaskStep
    target: str                  # ã‚»ãƒ¬ã‚¯ã‚¿/URL/ãƒ†ã‚­ã‚¹ãƒˆ
    expected_result: str         # æœŸå¾…çµæœ
    timeout_ms: int = 5000
    
    def to_dict(self) -> dict:
        return {
            "type": self.step_type.value,
            "target": self.target,
            "expected": self.expected_result,
            "timeout": self.timeout_ms,
        }


@dataclass
class BenchmarkScenario:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ãƒŠãƒªã‚ª"""
    name: str
    scenario_type: ScenarioType
    app_name: str
    steps: list[BenchmarkStep] = field(default_factory=list)
    description: str = ""
    
    def to_dict(self) -> dict:
        return {
            "name": self.name,
            "type": self.scenario_type.value,
            "app": self.app_name,
            "steps": [s.to_dict() for s in self.steps],
        }


@dataclass
class StepResult:
    """ã‚¹ãƒ†ãƒƒãƒ—çµæœ"""
    step_index: int
    success: bool
    duration_ms: int
    confidence_before: float
    confidence_after: float
    probe_count: int           # è¦³æ¸¬å›æ•°
    probe_types: list[str]     # è¦³æ¸¬ã‚¿ã‚¤ãƒ—ï¼ˆdom/uia/ssï¼‰
    error_message: str = ""
    
    @property
    def confidence_delta(self) -> float:
        return self.confidence_after - self.confidence_before
    
    @property
    def had_reprobe(self) -> bool:
        return self.probe_count > 1


@dataclass
class BenchmarkMetrics:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æŒ‡æ¨™"""
    # åŸºæœ¬
    total_steps: int = 0
    successful_steps: int = 0
    failed_steps: int = 0
    
    # ç¢ºåº¦
    avg_confidence: float = 0.0
    confidence_variance: float = 0.0
    belief_flips: int = 0        # ç¢ºåº¦ãŒåè»¢ã—ãŸå›æ•°
    
    # è¦³æ¸¬
    total_probes: int = 0
    cheap_probes: int = 0        # è»½é‡è¦³æ¸¬ï¼ˆDOMç­‰ï¼‰
    heavy_probes: int = 0        # é‡é‡è¦³æ¸¬ï¼ˆSSç­‰ï¼‰
    avg_probe_cost_ms: float = 0.0
    
    # æ™‚é–“
    total_duration_ms: int = 0
    avg_step_duration_ms: float = 0.0
    
    def success_rate(self) -> float:
        if self.total_steps == 0:
            return 0.0
        return self.successful_steps / self.total_steps
    
    def format(self) -> str:
        lines = [
            "ğŸ“Š Benchmark Metrics:",
            f"  æˆåŠŸç‡: {self.success_rate():.0%} ({self.successful_steps}/{self.total_steps})",
            f"  å¹³å‡ç¢ºåº¦: {self.avg_confidence:.0%}",
            f"  ç¢ºåº¦åè»¢: {self.belief_flips}å›",
            f"  è¦³æ¸¬å›æ•°: {self.total_probes} (è»½é‡:{self.cheap_probes} / é‡é‡:{self.heavy_probes})",
            f"  è¦³æ¸¬ã‚³ã‚¹ãƒˆ: {self.avg_probe_cost_ms:.0f}ms/å›",
            f"  ç·æ™‚é–“: {self.total_duration_ms}ms",
        ]
        return "\n".join(lines)


@dataclass
class BenchmarkRun:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œçµæœ"""
    scenario: BenchmarkScenario
    step_results: list[StepResult] = field(default_factory=list)
    metrics: BenchmarkMetrics = field(default_factory=BenchmarkMetrics)
    started_at: str = field(default_factory=lambda: datetime.now().isoformat())
    finished_at: Optional[str] = None
    
    def calculate_metrics(self) -> BenchmarkMetrics:
        """æŒ‡æ¨™ã‚’è¨ˆç®—"""
        m = BenchmarkMetrics()
        m.total_steps = len(self.step_results)
        
        if not self.step_results:
            return m
        
        confidences = []
        durations = []
        
        for sr in self.step_results:
            if sr.success:
                m.successful_steps += 1
            else:
                m.failed_steps += 1
            
            confidences.append(sr.confidence_after)
            durations.append(sr.duration_ms)
            m.total_probes += sr.probe_count
            
            # è»½é‡/é‡é‡åˆ†é¡
            for pt in sr.probe_types:
                if pt in ["dom", "uia"]:
                    m.cheap_probes += 1
                else:
                    m.heavy_probes += 1
            
            # ç¢ºåº¦åè»¢ãƒã‚§ãƒƒã‚¯
            if sr.confidence_delta < -0.2:
                m.belief_flips += 1
        
        m.avg_confidence = sum(confidences) / len(confidences)
        m.total_duration_ms = sum(durations)
        m.avg_step_duration_ms = m.total_duration_ms / len(durations)
        
        if m.total_probes > 0:
            m.avg_probe_cost_ms = m.total_duration_ms / m.total_probes
        
        self.metrics = m
        return m


class ScenarioBenchmark:
    """ã‚·ãƒŠãƒªã‚ªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    
    def __init__(self, output_dir: str = "."):
        self.output_dir = Path(output_dir)
        self.scenarios: list[BenchmarkScenario] = []
        self.runs: list[BenchmarkRun] = []
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚·ãƒŠãƒªã‚ªã‚’ç™»éŒ²
        self._register_sample_scenarios()
    
    def _register_sample_scenarios(self) -> None:
        """ã‚µãƒ³ãƒ—ãƒ«ã‚·ãƒŠãƒªã‚ªã‚’ç™»éŒ²"""
        # ChatGPT: æ—¢çŸ¥ãƒ•ãƒ­ãƒ¼
        self.scenarios.append(BenchmarkScenario(
            name="chatgpt_basic_chat",
            scenario_type=ScenarioType.KNOWN_FLOW,
            app_name="ChatGPT",
            description="ChatGPTã§åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆã‚’é€ä¿¡",
            steps=[
                BenchmarkStep(TaskStep.NAVIGATE, "https://chatgpt.com/", "ChatGPTç”»é¢è¡¨ç¤º"),
                BenchmarkStep(TaskStep.WAIT, "#prompt-textarea", "å…¥åŠ›æ¬„è¡¨ç¤º"),
                BenchmarkStep(TaskStep.INPUT, "#prompt-textarea", "ãƒ†ã‚¹ãƒˆå…¥åŠ›å®Œäº†"),
                BenchmarkStep(TaskStep.CLICK, "[data-testid='send-button']", "é€ä¿¡å®Œäº†"),
                BenchmarkStep(TaskStep.WAIT, ".response", "å¿œç­”å—ä¿¡"),
            ]
        ))
        
        # æ±ç”¨æ¤œç´¢: åˆè¦‹å¯¾å¿œãƒ†ã‚¹ãƒˆ
        self.scenarios.append(BenchmarkScenario(
            name="generic_search_flow",
            scenario_type=ScenarioType.UNKNOWN_FLOW,
            app_name="Generic",
            description="åˆè¦‹ã‚µã‚¤ãƒˆã§æ¤œç´¢ã‚’å®Ÿè¡Œ",
            steps=[
                BenchmarkStep(TaskStep.SEARCH, "search", "æ¤œç´¢æ¬„ç™ºè¦‹"),
                BenchmarkStep(TaskStep.INPUT, "search_input", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›"),
                BenchmarkStep(TaskStep.CLICK, "submit", "æ¤œç´¢å®Ÿè¡Œ"),
                BenchmarkStep(TaskStep.VERIFY, "results", "çµæœè¡¨ç¤º"),
            ]
        ))
    
    def run_scenario(
        self,
        scenario: BenchmarkScenario,
        executor: Optional[Callable] = None
    ) -> BenchmarkRun:
        """ã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ"""
        run = BenchmarkRun(scenario=scenario)
        
        for i, step in enumerate(scenario.steps):
            start = time.time()
            
            # ãƒ¢ãƒƒã‚¯å®Ÿè¡Œï¼ˆå®Ÿéš›ã«ã¯executorã‚’ä½¿ç”¨ï¼‰
            if executor:
                result = executor(step)
            else:
                result = self._mock_execute(step)
            
            duration = int((time.time() - start) * 1000)
            
            step_result = StepResult(
                step_index=i,
                success=result.get("success", False),
                duration_ms=duration,
                confidence_before=result.get("conf_before", 0.5),
                confidence_after=result.get("conf_after", 0.5),
                probe_count=result.get("probes", 1),
                probe_types=result.get("probe_types", ["dom"]),
                error_message=result.get("error", "")
            )
            
            run.step_results.append(step_result)
        
        run.finished_at = datetime.now().isoformat()
        run.calculate_metrics()
        
        self.runs.append(run)
        return run
    
    def _mock_execute(self, step: BenchmarkStep) -> dict:
        """ãƒ¢ãƒƒã‚¯å®Ÿè¡Œ"""
        import random
        
        time.sleep(0.05)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        return {
            "success": random.random() > 0.2,
            "conf_before": random.uniform(0.4, 0.7),
            "conf_after": random.uniform(0.5, 0.9),
            "probes": random.randint(1, 3),
            "probe_types": random.choices(["dom", "uia", "ss"], k=random.randint(1, 3)),
        }
    
    def compare_runs(self, run1: BenchmarkRun, run2: BenchmarkRun) -> dict:
        """2ã¤ã®å®Ÿè¡Œã‚’æ¯”è¼ƒ"""
        m1, m2 = run1.metrics, run2.metrics
        
        return {
            "success_rate_delta": m2.success_rate() - m1.success_rate(),
            "confidence_delta": m2.avg_confidence - m1.avg_confidence,
            "belief_flips_delta": m2.belief_flips - m1.belief_flips,
            "probe_cost_delta": m2.avg_probe_cost_ms - m1.avg_probe_cost_ms,
            "duration_delta": m2.total_duration_ms - m1.total_duration_ms,
        }
    
    def save_run(self, run: BenchmarkRun) -> Path:
        """å®Ÿè¡Œçµæœã‚’ä¿å­˜"""
        filename = f"bench_{run.scenario.name}_{run.started_at[:10]}.json"
        path = self.output_dir / filename
        
        data = {
            "scenario": run.scenario.to_dict(),
            "started_at": run.started_at,
            "finished_at": run.finished_at,
            "metrics": {
                "success_rate": run.metrics.success_rate(),
                "avg_confidence": run.metrics.avg_confidence,
                "belief_flips": run.metrics.belief_flips,
                "total_probes": run.metrics.total_probes,
                "total_duration_ms": run.metrics.total_duration_ms,
            },
            "step_results": [
                {
                    "index": sr.step_index,
                    "success": sr.success,
                    "duration_ms": sr.duration_ms,
                    "confidence_delta": sr.confidence_delta,
                }
                for sr in run.step_results
            ]
        }
        
        path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        return path
    
    def format_comparison(self, comparison: dict) -> str:
        """æ¯”è¼ƒçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        def arrow(delta: float) -> str:
            if delta > 0.05:
                return "â¬†ï¸"
            elif delta < -0.05:
                return "â¬‡ï¸"
            else:
                return "â¡ï¸"
        
        lines = [
            "ğŸ“ˆ Run Comparison:",
            f"  æˆåŠŸç‡: {arrow(comparison['success_rate_delta'])} {comparison['success_rate_delta']:+.0%}",
            f"  ç¢ºåº¦: {arrow(comparison['confidence_delta'])} {comparison['confidence_delta']:+.0%}",
            f"  ç¢ºåº¦åè»¢: {arrow(-comparison['belief_flips_delta'])} {comparison['belief_flips_delta']:+d}å›",
            f"  è¦³æ¸¬ã‚³ã‚¹ãƒˆ: {arrow(-comparison['probe_cost_delta'])} {comparison['probe_cost_delta']:+.0f}ms",
            f"  ç·æ™‚é–“: {arrow(-comparison['duration_delta'])} {comparison['duration_delta']:+d}ms",
        ]
        return "\n".join(lines)


# ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    print("=" * 60)
    print("Scenario Benchmark ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    bench = ScenarioBenchmark()
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚·ãƒŠãƒªã‚ªå®Ÿè¡Œ
    print("\n--- ã‚·ãƒŠãƒªã‚ªå®Ÿè¡Œ1 ---")
    scenario = bench.scenarios[0]
    run1 = bench.run_scenario(scenario)
    print(run1.metrics.format())
    
    # 2å›ç›®å®Ÿè¡Œï¼ˆæ¯”è¼ƒç”¨ï¼‰
    print("\n--- ã‚·ãƒŠãƒªã‚ªå®Ÿè¡Œ2 ---")
    run2 = bench.run_scenario(scenario)
    print(run2.metrics.format())
    
    # æ¯”è¼ƒ
    print("\n--- æ¯”è¼ƒ ---")
    comparison = bench.compare_runs(run1, run2)
    print(bench.format_comparison(comparison))
    
    print("\n" + "=" * 60)
    print("ãƒ†ã‚¹ãƒˆå®Œäº†")
