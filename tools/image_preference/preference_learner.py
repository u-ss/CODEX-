"""
å¥½ã¿å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³

ã€Œå¥½ã / ãã†ã§ã‚‚ãªã„ / å«Œã„ã€ãƒ•ã‚©ãƒ«ãƒ€ã®ç”»åƒã‚’èª­ã¿è¾¼ã¿ã€
ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†é¡å™¨ã‚’å­¦ç¿’ã™ã‚‹ã€‚
"""

import json
import pickle
import numpy as np
from pathlib import Path
from typing import Optional

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score

from feature_extractor import extract_features, features_to_vector, get_feature_keys


# ãƒ©ãƒ™ãƒ«å®šç¾©
LABEL_MAP = {
    "å¥½ã": 2,
    "ãã†ã§ã‚‚ãªã„": 1,
    "å«Œã„": 0,
}
LABEL_NAMES = {v: k for k, v in LABEL_MAP.items()}

# å¯¾å¿œç”»åƒæ‹¡å¼µå­
IMAGE_EXTENSIONS = {".jpg", ".jpeg", ".png", ".webp", ".bmp", ".gif", ".tiff"}


class PreferenceLearner:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç”»åƒå¥½ã¿ã‚’å­¦ç¿’ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚"""

    def __init__(self, training_dir: str | Path = "training_data"):
        self.training_dir = Path(training_dir)
        self.model: Optional[RandomForestClassifier] = None
        self.scaler: Optional[StandardScaler] = None
        self.feature_importances: Optional[dict] = None
        self.training_stats: dict = {}

    def scan_images(self) -> dict[str, list[Path]]:
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ç”»åƒãƒ‘ã‚¹ã‚’åé›†ã™ã‚‹ã€‚"""
        result = {}
        for label_name in LABEL_MAP:
            folder = self.training_dir / label_name
            if not folder.exists():
                result[label_name] = []
                continue
            images = [
                f for f in folder.iterdir()
                if f.is_file() and f.suffix.lower() in IMAGE_EXTENSIONS
            ]
            result[label_name] = sorted(images)
        return result

    def train(self, model_save_path: Optional[str | Path] = None) -> dict:
        """å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

        Args:
            model_save_path: ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆï¼ˆçœç•¥æ™‚ã¯ä¿å­˜ã—ãªã„ï¼‰

        Returns:
            å­¦ç¿’çµæœã®çµ±è¨ˆæƒ…å ±
        """
        image_map = self.scan_images()

        # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’åé›†
        X_list = []
        y_list = []
        file_paths = []

        for label_name, images in image_map.items():
            label = LABEL_MAP[label_name]
            for img_path in images:
                try:
                    features = extract_features(img_path)
                    vec = features_to_vector(features)
                    X_list.append(vec)
                    y_list.append(label)
                    file_paths.append(str(img_path))
                except Exception as e:
                    print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {img_path} ({e})")

        if len(X_list) < 3:
            raise ValueError(
                f"ç”»åƒãŒå°‘ãªã™ãã¾ã™ï¼ˆ{len(X_list)}æšï¼‰ã€‚å„ãƒ•ã‚©ãƒ«ãƒ€ã«æœ€ä½1æšãšã¤é…ç½®ã—ã¦ãã ã•ã„ã€‚"
            )

        X = np.array(X_list)
        y = np.array(y_list)

        # æ­£è¦åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)

        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå­¦ç¿’
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight="balanced",
        )
        self.model.fit(X_scaled, y)

        # ç‰¹å¾´é‡é‡è¦åº¦
        keys = get_feature_keys()
        importances = self.model.feature_importances_
        self.feature_importances = {
            keys[i]: float(importances[i])
            for i in range(len(keys))
        }

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒååˆ†ãªå ´åˆï¼‰
        cv_score = None
        if len(X_list) >= 10:
            n_splits = min(5, len(set(y)))
            if n_splits >= 2:
                scores = cross_val_score(self.model, X_scaled, y, cv=n_splits)
                cv_score = float(np.mean(scores))

        # çµ±è¨ˆæƒ…å ±
        self.training_stats = {
            "total_images": len(X_list),
            "per_label": {
                name: int(np.sum(y == label))
                for name, label in LABEL_MAP.items()
            },
            "feature_count": len(keys),
            "cv_accuracy": cv_score,
            "top_features": sorted(
                self.feature_importances.items(),
                key=lambda x: x[1],
                reverse=True,
            )[:10],
        }

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if model_save_path:
            save_path = Path(model_save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            with open(save_path, "wb") as f:
                pickle.dump({
                    "model": self.model,
                    "scaler": self.scaler,
                    "feature_importances": self.feature_importances,
                    "training_stats": self.training_stats,
                }, f)
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {save_path}")

        return self.training_stats

    @classmethod
    def load(cls, model_path: str | Path) -> "PreferenceLearner":
        """ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
        with open(model_path, "rb") as f:
            data = pickle.load(f)
        learner = cls()
        learner.model = data["model"]
        learner.scaler = data["scaler"]
        learner.feature_importances = data["feature_importances"]
        learner.training_stats = data.get("training_stats", {})
        return learner


if __name__ == "__main__":
    import sys
    training_dir = sys.argv[1] if len(sys.argv) > 1 else "training_data"
    model_path = sys.argv[2] if len(sys.argv) > 2 else "models/preference_model.pkl"

    learner = PreferenceLearner(training_dir)
    stats = learner.train(model_save_path=model_path)

    print("\nğŸ“Š å­¦ç¿’çµæœ:")
    print(f"  ç·ç”»åƒæ•°: {stats['total_images']}")
    for name, count in stats["per_label"].items():
        print(f"  {name}: {count}æš")
    if stats["cv_accuracy"]:
        print(f"  CVç²¾åº¦: {stats['cv_accuracy']:.1%}")
    print("\nğŸ”‘ é‡è¦ãªç‰¹å¾´é‡ TOP 10:")
    for name, imp in stats["top_features"]:
        print(f"  {name}: {imp:.4f}")
